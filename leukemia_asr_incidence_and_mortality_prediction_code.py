# -*- coding: utf-8 -*-
"""Leukemia ASR Incidence and Mortality Prediction Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lreLDSkL1EbgZuhcNx72N6XaITcTm88g

**LEUKEMIA PROJECT**

---
"""

import pandas as pd
import numpy as np
import random
import sklearn

np.random.seed(0)

#import data
leukemia_dataset = pd.read_csv("/content/leukemiadata.csv")
print(leukemia_dataset)

#DEFINE DATA
leukemia = leukemia_dataset.iloc[:,5:16]
leukemia.head()

#CHECK DATA TYPES
leukemia.info()

print(leukemia.describe())

"""**DATA PRE-PROCESSING**

---

**1. Missing Values Detection and Handling**
"""

#number of missing values
leukemia.isnull().sum()

#plot to visualize missing values
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(18,8))
colours = ['#34495E', 'seagreen']
sns.heatmap(leukemia.isnull(), cmap=sns.color_palette(colours))

#!pip install missingno
#import missingno as msno
#msno.matrix(leukemia, figsize=(10,5), fontsize=12, color=(0.375, 0.38, 0.45));

#msno.heatmap(leukemia, figsize=(10,5), fontsize=12);

#msno.dendrogram(leukemia)

#replace missing values with mean
leukemia.fillna(leukemia.mean(), inplace = True)
print(leukemia)

#replace missing values with median
#leukemia.fillna(leukemia.median(), inplace = True)
#print(leukemia)

#recheck
leukemia.isnull().sum()

"""**2. Outlier Detection and Handling**"""

#outlier detection
sns.set_theme(rc={'figure.figsize':(25,5)})
sns.set(font_scale=1.5)
plt.xticks(rotation=45)
sns.boxplot(leukemia)

#outlier handling
print("Old Shape: ", leukemia.shape)

#1) HDI Level
##using IQR
Q1_hdi = leukemia['HDI.Level'].quantile(0.25)
Q3_hdi = leukemia['HDI.Level'].quantile(0.75)
IQR_hdi = Q3_hdi - Q1_hdi
lower_hdi = Q1_hdi - 1.5*IQR_hdi
upper_hdi = Q3_hdi + 1.5*IQR_hdi
##Create arrays of Boolean values indicating the outlier rows
upper_array_hdi = np.where(leukemia['HDI.Level'] >= upper_hdi)[0]
lower_array_hdi = np.where(leukemia['HDI.Level'] <= lower_hdi)[0]
##Removing the outliers
leukemia.drop(index=leukemia.index[upper_array_hdi], inplace=True)
leukemia.drop(index=leukemia.index[lower_array_hdi], inplace=True)

#2) Health Expenditure
##using IQR
Q1_healthexp = leukemia['Health.Exp'].quantile(0.25)
Q3_healthexp = leukemia['Health.Exp'].quantile(0.75)
IQR_healthexp = Q3_healthexp - Q1_healthexp
lower_healthexp = Q1_healthexp - 1.5*IQR_healthexp
upper_healthexp = Q3_healthexp + 1.5*IQR_healthexp
##Create arrays of Boolean values indicating the outlier rows
upper_array_healthexp = np.where(leukemia['Health.Exp'] >= upper_healthexp)[0]
lower_array_healthexp = np.where(leukemia['Health.Exp'] <= lower_healthexp)[0]
##Removing the outliers
leukemia.drop(index=leukemia.index[upper_array_healthexp], inplace=True)
leukemia.drop(index=leukemia.index[lower_array_healthexp], inplace=True)

#3) GDP
##using IQR
Q1_gdp = leukemia['GDP'].quantile(0.25)
Q3_gdp = leukemia['GDP'].quantile(0.75)
IQR_gdp = Q3_gdp - Q1_gdp
lower_gdp = Q1_gdp - 1.5*IQR_gdp
upper_gdp = Q3_gdp + 1.5*IQR_gdp
##Create arrays of Boolean values indicating the outlier rows
upper_array_gdp = np.where(leukemia['GDP'] >= upper_gdp)[0]
lower_array_gdp = np.where(leukemia['GDP'] <= lower_gdp)[0]
##Removing the outliers
leukemia.drop(index=leukemia.index[upper_array_gdp], inplace=True)
leukemia.drop(index=leukemia.index[lower_array_gdp], inplace=True)

#4) Overweight Age Below 5
##using IQR
Q1_oab5 = leukemia['overweight.age.below.five'].quantile(0.25)
Q3_oab5 = leukemia['overweight.age.below.five'].quantile(0.75)
IQR_oab5 = Q3_oab5 - Q1_oab5
lower_oab5 = Q1_oab5 - 1.5*IQR_oab5
upper_oab5 = Q3_oab5 + 1.5*IQR_oab5
##Create arrays of Boolean values indicating the outlier rows
upper_array_oab5 = np.where(leukemia['overweight.age.below.five'] >= upper_oab5)[0]
lower_array_oab5 = np.where(leukemia['overweight.age.below.five'] <= lower_oab5)[0]
##Removing the outliers
leukemia.drop(index=leukemia.index[upper_array_oab5], inplace=True)
leukemia.drop(index=leukemia.index[lower_array_oab5], inplace=True)

#5) Not using safe water
##using IQR
Q1_nusw = leukemia['not.using.safe.water'].quantile(0.25)
Q3_nusw = leukemia['not.using.safe.water'].quantile(0.75)
IQR_nusw = Q3_nusw - Q1_nusw
lower_nusw = Q1_nusw - 1.5*IQR_nusw
upper_nusw = Q3_nusw + 1.5*IQR_nusw
##Create arrays of Boolean values indicating the outlier rows
upper_array_nusw = np.where(leukemia['not.using.safe.water'] >= upper_nusw)[0]
lower_array_nusw = np.where(leukemia['not.using.safe.water'] <= lower_nusw)[0]
##Removing the outliers
leukemia.drop(index=leukemia.index[upper_array_nusw], inplace=True)
leukemia.drop(index=leukemia.index[lower_array_nusw], inplace=True)

#6) Stunting
##using IQR
Q1_stunting = leukemia['Stunting'].quantile(0.25)
Q3_stunting = leukemia['Stunting'].quantile(0.75)
IQR_stunting = Q3_stunting - Q1_stunting
lower_stunting = Q1_stunting - 1.5*IQR_stunting
upper_stunting = Q3_stunting + 1.5*IQR_stunting
##Create arrays of Boolean values indicating the outlier rows
upper_array_stunting = np.where(leukemia['Stunting'] >= upper_stunting)[0]
lower_array_stunting = np.where(leukemia['Stunting'] <= lower_stunting)[0]
##Removing the outliers
leukemia.drop(index=leukemia.index[upper_array_stunting], inplace=True)
leukemia.drop(index=leukemia.index[lower_array_stunting], inplace=True)

#7) Overweight Age Five to Nineteen
##using IQR
Q1_oa5t9 = leukemia['overweight.age.five.to.nineteen'].quantile(0.25)
Q3_oa5t9 = leukemia['overweight.age.five.to.nineteen'].quantile(0.75)
IQR_oa5t9 = Q3_oa5t9 - Q1_oa5t9
lower_oa5t9 = Q1_oa5t9 - 1.5*IQR_oa5t9
upper_oa5t9 = Q3_oa5t9 + 1.5*IQR_oa5t9
##Create arrays of Boolean values indicating the outlier rows
upper_array_oa5t9 = np.where(leukemia['overweight.age.five.to.nineteen'] >= upper_oa5t9)[0]
lower_array_oa5t9 = np.where(leukemia['overweight.age.five.to.nineteen'] <= lower_oa5t9)[0]
##Removing the outliers
leukemia.drop(index=leukemia.index[upper_array_oa5t9], inplace=True)
leukemia.drop(index=leukemia.index[lower_array_oa5t9], inplace=True)

#7) UV
##using IQR
Q1_uv = leukemia['UV'].quantile(0.25)
Q3_uv = leukemia['UV'].quantile(0.75)
IQR_uv = Q3_uv - Q1_uv
lower_uv = Q1_uv - 1.5*IQR_uv
upper_uv = Q3_uv + 1.5*IQR_uv
##Create arrays of Boolean values indicating the outlier rows
upper_array_uv = np.where(leukemia['UV'] >= upper_uv)[0]
lower_array_uv = np.where(leukemia['UV'] <= lower_uv)[0]
##Removing the outliers
leukemia.drop(index=leukemia.index[upper_array_uv], inplace=True)
leukemia.drop(index=leukemia.index[lower_array_uv], inplace=True)

# Print the new shape of the DataFrame
print("New Shape: ", leukemia.shape)

sns.set_theme(rc={'figure.figsize':(25,5)})
sns.set(font_scale=1.5)
plt.xticks(rotation=45)
sns.boxplot(leukemia)

"""**3. Feature Selection**"""

#based on correlation
##generating correlation matrix
leukemia_corr = leukemia.corr()
leukemia_corr.head()

#the correlation heatmap
sns.set_theme(rc={'figure.figsize':(8,5)})
sns.heatmap(leukemia_corr, annot=True, linewidths=.5, fmt= '.1f')
plt.title('Heatmap Correlation')
plt.show()

#compare the correlation between features and remove correlation yg >= 0.9
columns = np.full((leukemia_corr.shape[0],), True, dtype=bool)
for i in range(leukemia_corr.shape[0]):
    for j in range(i+1, leukemia_corr.shape[0]):
        if leukemia_corr.iloc[i,j] >= 0.9:
            if columns[j]:
                columns[j] = False

selected_columns = leukemia.columns[columns]
selected_columns.shape

leukemia1 = leukemia[selected_columns]
leukemia1.head()

##the dataset has only those columns with correlation less than 0.9

result = pd.DataFrame()
result['ASR.inc'] = leukemia1.iloc[:,0]
result['ASR.mort'] = leukemia1.iloc[:,1]
leukemia1.head()

#using p-value
from scipy.stats import pearsonr
sns.set_theme(rc={'figure.figsize':(10,8)})
#H0: there is no linear correlation between the two variables.
#Ha: there is a linear correlation between the two variables.
##if p-value < 0.05, reject the null hypothesis

#ASR Incidence vs ASR Mortality
pearsonr(leukemia['ASR.mort'],leukemia['ASR.inc'])
#reject the null hypothesis
#there is a significant linear correlation between the two variables.

import seaborn as sns
sns.scatterplot(x=leukemia['ASR.mort'], y=leukemia['ASR.inc'], data=leukemia)

#ASR Incidence vs HDI Level
pearsonr(leukemia['HDI.Level'],leukemia['ASR.inc'])
#reject the null hypothesis
#there is a significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['HDI.Level'], y=leukemia['ASR.inc'], data=leukemia)

#ASR Incidence vs Health Exp
pearsonr(leukemia['Health.Exp'],leukemia['ASR.inc'])
#reject the null hypothesis
#there is a significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['Health.Exp'], y=leukemia['ASR.inc'], data=leukemia)

#ASR Incidence vs GDP
pearsonr(leukemia['GDP'],leukemia['ASR.inc'])
#can't reject the null hypothesis
#there is no significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['GDP'], y=leukemia['ASR.inc'], data=leukemia)

#ASR Incidence vs Overweight Age Below Five
pearsonr(leukemia['overweight.age.below.five'],leukemia['ASR.inc'])
#reject the null hypothesis
#there is a significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['overweight.age.below.five'], y=leukemia['ASR.inc'], data=leukemia)

#ASR Incidence vs Not Using Safe Water
pearsonr(leukemia['not.using.safe.water'],leukemia['ASR.inc'])
#reject the null hypothesis
#there is a significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['not.using.safe.water'], y=leukemia['ASR.inc'], data=leukemia)

#ASR Incidence vs Stunting
pearsonr(leukemia['Stunting'],leukemia['ASR.inc'])
#reject the null hypothesis
#there is a significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['Stunting'], y=leukemia['ASR.inc'], data=leukemia)

#ASR Incidence vs Overweight Age Five to Nineteen
pearsonr(leukemia['overweight.age.five.to.nineteen'],leukemia['ASR.inc'])
#can't reject the null hypothesis
#there is no significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['overweight.age.five.to.nineteen'], y=leukemia['ASR.inc'], data=leukemia)

#ASR Incidence vs UV
pearsonr(leukemia['UV'],leukemia['ASR.inc'])
#reject the null hypothesis
#there is a significant linear correlation between the two variables

sns.scatterplot(x=leukemia['UV'], y=leukemia['ASR.inc'], data=leukemia)

#conclusion: drop GDP, overweight.age.five.to.nineteen to predict the ASR.inc

#ASR Mortality vs HDI Level
pearsonr(leukemia['HDI.Level'],leukemia['ASR.mort'])
#can't reject the null hypothesis
#there is no significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['HDI.Level'], y=leukemia['ASR.mort'], data=leukemia)

#ASR Mortality vs Health Exp
pearsonr(leukemia['Health.Exp'],leukemia['ASR.mort'])
#can't reject the null hypothesis
#there is no significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['Health.Exp'], y=leukemia['ASR.mort'], data=leukemia)

#ASR Mortality vs GDP
pearsonr(leukemia['GDP'],leukemia['ASR.mort'])
#can't reject the null hypothesis
#there is no significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['GDP'], y=leukemia['ASR.mort'], data=leukemia)

#ASR Mortality vs Overweight Age Below Five
pearsonr(leukemia['overweight.age.below.five'],leukemia['ASR.mort'])
#can't reject the null hypothesis
#there is no significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['overweight.age.below.five'], y=leukemia['ASR.mort'], data=leukemia)

#ASR Mortality vs Not Using Safe Water
pearsonr(leukemia['not.using.safe.water'],leukemia['ASR.mort'])
#can't reject the null hypothesis
#there is no significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['not.using.safe.water'], y=leukemia['ASR.mort'], data=leukemia)

#ASR Mortality vs Stunting
pearsonr(leukemia['Stunting'],leukemia['ASR.mort'])
#can't reject the null hypothesis
#there is no significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['Stunting'], y=leukemia['ASR.mort'], data=leukemia)

#ASR Mortality vs Overweight Age Five to Nineteen
pearsonr(leukemia['overweight.age.five.to.nineteen'],leukemia['ASR.mort'])
#reject the null hypothesis
#there is a significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['overweight.age.five.to.nineteen'], y=leukemia['ASR.mort'], data=leukemia)

#ASR Mortality vs UV
pearsonr(leukemia['UV'],leukemia['ASR.mort'])
#reject the null hypothesis
#there is a significant linear correlation between the two variables.

sns.scatterplot(x=leukemia['UV'], y=leukemia['ASR.mort'], data=leukemia)

#conclusion: drop HDI.Level, Health.Exp, GDP, overweight.age.below.five, not.using.safe.water, and Stunting
#            to predict the ASR.mort

"""Define New Dataset for both ASR Incidence and Mortality Prediction"""

#dataframe for ASR.inc model: drop GDP and overweight.age.five.to.nineteen to predict the ASR.inc
leukemia1 = leukemia.drop(columns = ['GDP','overweight.age.five.to.nineteen'],axis=1)
leukemia1

#dataframe for ASR.mort model: drop HDI.Level, Health.Exp, GDP, overweight.age.below.five, not.using.safe.water, and Stunting
leukemia2 = leukemia.drop(columns = ['HDI.Level','Health.Exp','GDP','overweight.age.below.five','not.using.safe.water','Stunting'],axis=1)
leukemia2

"""5. Data Normalization"""

# Specify variables to exclude
variables_to_exclude = ['ASR.inc', 'ASR.mort']

# Copy the data excluding specified variables
df_min_max_scaled1 = leukemia1.drop(variables_to_exclude, axis=1).copy()

# Apply min-max scaling techniques
for column in df_min_max_scaled1.columns:
    df_min_max_scaled1[column] = (df_min_max_scaled1[column] - df_min_max_scaled1[column].min()) / (df_min_max_scaled1[column].max() - df_min_max_scaled1[column].min())

# View normalized data
print(df_min_max_scaled1)

leukemia1 = pd.concat([leukemia.iloc[:,0:2],df_min_max_scaled1], axis=1)
leukemia1

# Specify variables to exclude
variables_to_exclude = ['ASR.inc', 'ASR.mort']

# Copy the data excluding specified variables
df_min_max_scaled2 = leukemia2.drop(variables_to_exclude, axis=1).copy()

# Apply min-max scaling techniques
for column in df_min_max_scaled2.columns:
    df_min_max_scaled2[column] = (df_min_max_scaled2[column] - df_min_max_scaled2[column].min()) / (df_min_max_scaled2[column].max() - df_min_max_scaled2[column].min())

# View normalized data
print(df_min_max_scaled2)

leukemia2 = pd.concat([leukemia.iloc[:,0:2],df_min_max_scaled2], axis=1)
leukemia2

"""**MODEL BUILDING**

---

**1. Data Splitting (Train & Test)**
"""

#split data into train and test
np.random.seed(0)
##ASR.inc
index1 = np.random.choice(leukemia1.index, size=round(0.80 * len(leukemia1)), replace=False)
train1 = leukemia1.loc[index1]
test1 = leukemia1.loc[~leukemia1.index.isin(index1)]

print(train1)

print(test1)

np.random.seed(0)
##ASR.mort
index2 = np.random.choice(leukemia2.index, size=round(0.80 * len(leukemia2)), replace=False)
train2 = leukemia2.loc[index2]
test2 = leukemia2.loc[~leukemia2.index.isin(index2)]

print(train2)

print(test2)

#split into y_train, x_train, y_test, x_test
##y1_train = ASR.inc
y1_train = train1.iloc[:,0]
y1_train.head()

##y2_train = ASR.mort
y2_train = train2.iloc[:,1]
y2_train.head()

##x_train
#ASR.inc
x1_train = train1.iloc[:,2:9]
x1_train

#ASR.mort
x2_train = pd.concat([train2.iloc[:,0], train2.iloc[:,2:5]], axis=1)
x2_train

##y_test
##y1_test = ASR.inc
y1_test = test1.iloc[:,0]
y1_test.head()

##y2_test = ASR.mort
y2_test = test2.iloc[:,1]
y2_test

##x_test
#ASR.inc
x1_test = test1.iloc[:,2:9]
x1_test

#ASR.mort
x2_test = pd.concat([test2.iloc[:,0], test2.iloc[:,2:5]], axis=1)
x2_test

"""**2. Model Fitting**

***Linear Model***
"""

#from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

#ASR.inc
lm1 = sm.OLS(y1_train, x1_train)
lm_model1 = lm1.fit()
print(lm_model1.summary())

#ASR.Mort
lm2 = sm.OLS(y2_train, x2_train)
lm_model2 = lm2.fit()
print(lm_model2.summary())

"""***Generalized Linear Model***"""

import statsmodels.api as sm
#choose Gaussian family for the generalized linear model

#ASR.inc
glm3 = sm.GLM(y1_train, x1_train, family=sm.families.Gaussian())
gl_model1 = glm3.fit()
print(gl_model1.summary())

#ASR.mort
glm9 = sm.GLM(y2_train, x2_train, family=sm.families.Gaussian())
gl_model2 = glm9.fit()
print(gl_model2.summary())

"""***Regression Tree***"""

#for hyperparameter tuning
from sklearn.model_selection import GridSearchCV
#for regression tree
from sklearn.tree import DecisionTreeRegressor

#ASR.inc
max_depth = [1,4,7]
min_samples_split = [2,3,4]
min_samples_leaf = [2,4,13,14]
splitter = ['random', 'best']
max_features = ['auto', 'sqrt', 'log2']
criterion =['friedman_mse', 'squared_error', 'poisson', 'absolute_error']

param_grid = {
   'max_depth' : max_depth,
   'min_samples_split' : min_samples_split,
   'min_samples_leaf' : min_samples_leaf,
   'splitter' : splitter,
   'max_features' : max_features,
   'criterion' : criterion
}

rt_model1 = GridSearchCV(estimator = DecisionTreeRegressor(random_state=0),
                           param_grid = param_grid,
                           scoring='neg_mean_absolute_error',
                           cv=5,
                           error_score='raise'
                         )

rt_model1.fit(x1_train, y1_train)
rtbest_params1 = rt_model1.best_params_

rtbest_params1

#visualisasi
from sklearn.tree import plot_tree
best_rt_model1 = rt_model1.best_estimator_
plt.figure(figsize=(10,8), dpi=150)
plot_tree(best_rt_model1, feature_names=x1_train.columns)

#ASR.mort

max_depth = [1,3,4,6,9,11]
min_samples_split = [2,3,4]
min_samples_leaf = [2,4,13,14]
splitter = ['random', 'best']
max_features = ['auto', 'sqrt', 'log2']
criterion =['friedman_mse', 'squared_error', 'poisson', 'absolute_error']

param_grid = {
   'max_depth' : max_depth,
   'min_samples_split' : min_samples_split,
   'min_samples_leaf' : min_samples_leaf,
   'splitter' : splitter,
   'max_features' : max_features,
   'criterion' : criterion
}

rt_model2 = GridSearchCV(estimator = DecisionTreeRegressor(random_state=0),
                           param_grid = param_grid,
                           scoring='neg_mean_absolute_error',
                           cv=5,
                           error_score='raise'
                         )

rt_model2.fit(x2_train, y2_train)
rtbest_params2 = rt_model2.best_params_

rtbest_params2

#visualisasi
best_rt_model2 = rt_model2.best_estimator_
plt.figure(figsize=(100,50), dpi=150)
plot_tree(best_rt_model2, feature_names=x2_train.columns)

"""***Random Forest***"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

#ASR.inc

param_grid = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110, 120, 150],
    'max_features': [2, 3, 4, 5],
    'min_samples_leaf': [3, 4, 5, 7, 10],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [100, 200, 300, 1000]
}

rf_model1 = GridSearchCV(estimator = RandomForestRegressor(random_state=0),
                           param_grid = param_grid,
                           cv = 3,
                           n_jobs = -1,
                           verbose = 2)

rf_model1.fit(x1_train, y1_train)
rfbest_params1 = rf_model1.best_params_

rfbest_params1

best_rf_model1 = rf_model1.best_estimator_

import pydot
#pull out one tree from the forest
Tree1 = best_rf_model1[5]

#export the image to a dot file
from sklearn import tree
plt.figure(figsize=(25,15))
tree.plot_tree(Tree1, filled = True,
               rounded = True,
               fontsize=14,
               feature_names=x1_train.columns)

#ASR.mort

param_grid = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110, 120, 150],
    'max_features': [2, 3, 4, 5],
    'min_samples_leaf': [3, 4, 5, 7, 10],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [100, 200, 300, 1000]
}

rf_model2 = GridSearchCV(estimator = RandomForestRegressor(random_state=0),
                           param_grid = param_grid,
                           cv = 3,
                           n_jobs = -1,
                           verbose = 2)

rf_model2.fit(x2_train, y2_train)
rfbest_params2 = rf_model2.best_params_

rfbest_params2

best_rf_model2 = rf_model2.best_estimator_

import pydot

#pull out one tree from the forest
Tree = best_rf_model2.estimators_[5]

#export the image to a dot file
from sklearn import tree
plt.figure(figsize=(25,15))
tree.plot_tree(Tree, filled = True,
               rounded = True,
               fontsize=12,
               feature_names=x2_train.columns)

"""***XGBoost***"""

from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor

#ASR.inc
from sklearn.model_selection import GridSearchCV

learning_rate = [0.01, 0.1, 0.2]
n_estimators = [100, 150, 200]
min_child_weight = [1, 3, 5]
gamma = [0.1, 0.2, 0.5]
subsample = [0.5, 0.7, 1, 1.5]
alpha = [1, 1.5, 2, 2.5]
lambda_val = [1.5, 2]
max_depth = [1,5,7]
colsample_bytree = [0.1,0.5,1]

param_grid = {
    'learning_rate': learning_rate,
    'n_estimators': n_estimators,
    'min_child_weight': min_child_weight,
    'gamma': gamma,
    'subsample': subsample,
    'alpha': alpha,
    'lambda': lambda_val,
    'max_depth': max_depth,
    'colsample_bytree':colsample_bytree
}

xgb_model1 = GridSearchCV(estimator=XGBRegressor(random_state=0,
                                                 n_jobs=-1),
                           param_grid=param_grid,
                           scoring='neg_mean_absolute_error',
                           cv=5,
                           n_jobs=-1)

xgb_model1.fit(x1_train, y1_train)
xgbbest_params1 = xgb_model1.best_params_

xgbbest_params1

from xgboost import XGBRegressor
from xgboost import plot_tree
import matplotlib.pyplot as plt

# Assuming grid_search is your GridSearchCV object
# Access the best estimator
best_xgb_model1 = xgb_model1.best_estimator_

# Plot the first tree of the best XGBoost model
plot_tree(best_xgb_model1, num_trees=2)
plt.show()

#2. model fitting using y2_train = ASR.mort

learning_rate = [0.01, 0.1]
n_estimators = [100, 130, 150, 200]
min_child_weight = [1, 3, 5, 7]
gamma = [0, 0.01, 0.1]
subsample = [0.3, 0.5, 0.7]
alpha = [1, 1.5, 2, 2.5]
lambda_val = [1, 1.5, 2, 2.5]

param_grid = {
    'learning_rate': learning_rate,
    'n_estimators': n_estimators,
    'min_child_weight': min_child_weight,
    'gamma': gamma,
    'subsample': subsample,
    'alpha': alpha,
    'lambda': lambda_val
}

xgb_model2 = GridSearchCV(estimator=XGBRegressor(max_depth=3,
                                                 colsample_bytree=0.5,
                                                 n_jobs=-1),
                           param_grid=param_grid,
                           scoring='neg_mean_absolute_error',
                           cv=5,
                           n_jobs=-1)

xgb_model2.fit(x2_train, y2_train)
xgbbest_params2 = xgb_model2.best_params_

xgbbest_params2

# Assuming grid_search is your GridSearchCV object
# Access the best estimator
best_xgb_model2 = xgb_model2.best_estimator_

# Plot the first tree of the best XGBoost model
plot_tree(best_xgb_model2, num_trees=0)
plt.show()

"""**3. Model Prediction**

***Linear Model***
"""

#ASR.inc (train)
y1_tpred_lm = lm_model1.predict(x1_train)
y1_tpred_lm

plt.scatter(y1_train, y1_tpred_lm)
plt.xlabel('Actual ASR.inc')
plt.ylabel('Predicted ASR.inc')
plt.title('Actual vs Predicted ASR.inc using Linear Model (Train)')
plt.show()

#ASR.inc
y1_pred_lm = lm_model1.predict(x1_test)
y1_pred_lm

plt.scatter(y1_test, y1_pred_lm)
plt.xlabel('Actual ASR.inc')
plt.ylabel('Predicted ASR.inc')
plt.title('Actual vs Predicted ASR.inc using Linear Model')
plt.show()

#ASR.Mort (train)
y2_tpred_lm = lm_model2.predict(x2_train)
y2_tpred_lm

plt.scatter(y2_train, y2_tpred_lm)
plt.xlabel('Actual ASR.mort')
plt.ylabel('Predicted ASR.mort')
plt.title('Actual vs Predicted ASR.mort using Linear Model (Train)')
plt.show()

#ASR.Mort
y2_pred_lm = lm_model2.predict(x2_test)
y2_pred_lm

plt.scatter(y2_test, y2_pred_lm)
plt.xlabel('Actual ASR.mort')
plt.ylabel('Predicted ASR.mort')
plt.title('Actual vs Predicted ASR.mort using Linear Model')
plt.show()

"""***Generalized Linear Model***"""

#ASR.inc (train)
y1_tpred_glm= gl_model1.predict(x1_train)
y1_tpred_glm

plt.scatter(y1_train, y1_tpred_glm)
plt.xlabel('Actual ASR.inc')
plt.ylabel('Predicted ASR.inc')
plt.title('Actual vs Predicted ASR.inc using GLM (Train)')
plt.show()

#ASR.inc
y1_pred_glm= gl_model1.predict(x1_test)
y1_pred_glm

plt.scatter(y1_test, y1_pred_glm)
plt.xlabel('Actual ASR.inc')
plt.ylabel('Predicted ASR.inc')
plt.title('Actual vs Predicted ASR.inc using GLM')
plt.show()

#ASR.mort (train)
y2_tpred_glm= gl_model2.predict(x2_train)
y2_tpred_glm

plt.scatter(y2_train, y2_tpred_glm)
plt.xlabel('Actual ASR.mort')
plt.ylabel('Predicted ASR.mort')
plt.title('Actual vs Predicted ASR.mort using GLM (Train)')
plt.show()

#ASR.mort
y2_pred_glm= gl_model2.predict(x2_test)
y2_pred_glm

plt.scatter(y2_test, y2_pred_glm)
plt.xlabel('Actual ASR.mort')
plt.ylabel('Predicted ASR.mort')
plt.title('Actual vs Predicted ASR.mort using GLM')
plt.show()

"""***Regression Tree***"""

#ASR.inc (train)
y1_tpred_rt= best_rt_model1.predict(x1_train)
y1_tpred_rt

plt.scatter(y1_train, y1_tpred_rt)
plt.xlabel('Actual ASR.inc')
plt.ylabel('Predicted ASR.inc')
plt.title('Actual vs Predicted ASR.inc using Regression Tree (Train)')
plt.show()

#ASR.inc
y1_pred_rt= best_rt_model1.predict(x1_test)
y1_pred_rt

plt.scatter(y1_test, y1_pred_rt)
plt.xlabel('Actual ASR.inc')
plt.ylabel('Predicted ASR.inc')
plt.title('Actual vs Predicted ASR.inc using Regression Tree')
plt.show()

#ASR.mort (train)
y2_tpred_rt= best_rt_model2.predict(x2_train)
y2_tpred_rt

plt.scatter(y2_train, y2_tpred_rt)
plt.xlabel('Actual ASR.mort')
plt.ylabel('Predicted ASR.mort')
plt.title('Actual vs Predicted ASR.mort using Regression Tree (Train)')
plt.show()

#ASR.mort
y2_pred_rt= best_rt_model2.predict(x2_test)
y2_pred_rt

plt.scatter(y2_test, y2_pred_rt)
plt.xlabel('Actual ASR.mort')
plt.ylabel('Predicted ASR.mort')
plt.title('Actual vs Predicted ASR.mort using Regression Tree')
plt.show()

"""***Random Forest***"""

#ASR.inc (train)
y1_tpred_rf= best_rf_model1.predict(x1_train)
y1_tpred_rf

plt.scatter(y1_train, y1_tpred_rf)
plt.xlabel('Actual ASR.inc')
plt.ylabel('Predicted ASR.inc')
plt.title('Actual vs Predicted ASR.inc using Random Forest (Train)')
plt.show()

#ASR.inc
y1_pred_rf= best_rf_model1.predict(x1_test)
y1_pred_rf

plt.scatter(y1_test, y1_pred_rf)
plt.xlabel('Actual ASR.inc')
plt.ylabel('Predicted ASR.inc')
plt.title('Actual vs Predicted ASR.inc using Random Forest')
plt.show()

#ASR.mort (Train)
y2_tpred_rf= best_rf_model2.predict(x2_train)
y2_tpred_rf

plt.scatter(y2_train, y2_tpred_rf)
plt.xlabel('Actual ASR.mort')
plt.ylabel('Predicted ASR.mort')
plt.title('Actual vs Predicted ASR.mort using Random Forest (Train)')
plt.show()

#ASR.mort
y2_pred_rf= best_rf_model2.predict(x2_test)
y2_pred_rf

plt.scatter(y2_test, y2_pred_rf)
plt.xlabel('Actual ASR.mort')
plt.ylabel('Predicted ASR.mort')
plt.title('Actual vs Predicted ASR.mort using Random Forest')
plt.show()

"""***XGBoost***"""

#Asr.inc (Train)
y1_tpred_xgb = best_xgb_model1.predict(x1_train)
y1_tpred_xgb

plt.scatter(y1_train, y1_tpred_xgb)
plt.xlabel('Actual ASR.inc')
plt.ylabel('Predicted ASR.inc')
plt.title('Actual vs Predicted ASR.inc using XGBoost (Train)')
plt.show()

#Asr.inc
y1_pred_xgb = best_xgb_model1.predict(x1_test)
y1_pred_xgb

plt.scatter(y1_test, y1_pred_xgb)
plt.xlabel('Actual ASR.inc')
plt.ylabel('Predicted ASR.inc')
plt.title('Actual vs Predicted ASR.inc using XGBoost')
plt.show()

#Asr.mort (Train)
y2_tpred_xgb= best_xgb_model2.predict(x2_train)
y2_tpred_xgb

plt.scatter(y2_train, y2_tpred_xgb)
plt.xlabel('Actual ASR.mort')
plt.ylabel('Predicted ASR.mort')
plt.title('Actual vs Predicted ASR.mort using XGBoost (Train)')
plt.show()

#Asr.mort
y2_pred_xgb= best_xgb_model2.predict(x2_test)
y2_pred_xgb

plt.scatter(y2_test, y2_pred_xgb)
plt.xlabel('Actual ASR.mort')
plt.ylabel('Predicted ASR.mort')
plt.title('Actual vs Predicted ASR.mort using XGBoost')
plt.show()

"""**MODEL EVALUATION**"""

#Using RMSE (Root Mean Square Error )
#Low RMSE values show that the model makes more accurate predictions and fits the data well.
from sklearn.metrics import mean_squared_error

"""***LM (Train)***"""

#ASR.INC
t_rmse_inc_lm = np.sqrt(mean_squared_error(y1_train, y1_tpred_lm))
print(t_rmse_inc_lm)

##using MAPE
t_mape_inc_lm = np.mean(np.abs((y1_train - y1_tpred_lm)/y1_train))*100
print(t_mape_inc_lm)

##using MAAPE
t_maape_inc_lm = np.mean(np.abs((y1_train - y1_tpred_lm) / ((y1_train + y1_tpred_lm) / 2))) * 100
print(t_maape_inc_lm)

##using R-MAPE
t_rmape_inc_lm = np.mean(np.abs((y1_train - y1_tpred_lm) / np.maximum(y1_train, y1_tpred_lm))) * 100
print(t_rmape_inc_lm)

##check R-square
from sklearn.metrics import r2_score
t_r2_inc_lm = r2_score(y1_train, y1_tpred_lm)
print(t_r2_inc_lm)

##check R-square
t_r2_inc_lm1 = np.corrcoef(y1_train, y1_tpred_lm)[0,1]**2
print(t_r2_inc_lm1)

#ASR.Mort
t_rmse_mort_lm = np.sqrt(mean_squared_error(y2_train, y2_tpred_lm))
print(t_rmse_mort_lm)

##using MAPE
t_mape_mort_lm = np.mean(np.abs((y2_train - y2_tpred_lm)/y2_train))*100
print(t_mape_mort_lm)

##using MAAPE
t_maape_mort_lm = np.mean(np.abs((y2_train - y2_tpred_lm) / ((y2_train + y2_tpred_lm) / 2))) * 100
print(t_maape_mort_lm)

##using R-MAPE
t_rmape_mort_lm = np.mean(np.abs((y2_train - y2_tpred_lm) / np.maximum(y2_train, y2_tpred_lm))) * 100
print(t_rmape_mort_lm)

##check R-square
from sklearn.metrics import r2_score
t_r2_mort_lm = r2_score(y2_train, y2_tpred_lm)
print(t_r2_mort_lm)

##check R-square
t_r2_mort_lm1 = np.corrcoef(y2_train, y2_tpred_lm)[0,1]**2
print(t_r2_mort_lm1)

"""***LM (Test)***"""

#ASR.INC
rmse_inc_lm = np.sqrt(mean_squared_error(y1_test, y1_pred_lm))
print(rmse_inc_lm)

##using MAPE
mape_inc_lm = np.mean(np.abs((y1_test - y1_pred_lm)/y1_test))*100
print(mape_inc_lm)

##using MAAPE
maape_inc_lm = np.mean(np.abs((y1_test - y1_pred_lm) / ((y1_test + y1_pred_lm) / 2))) * 100
print(maape_inc_lm)

##using R-MAPE
rmape_inc_lm = np.mean(np.abs((y1_test - y1_pred_lm) / np.maximum(y1_test, y1_pred_lm))) * 100
print(rmape_inc_lm)

##check R-square
from sklearn.metrics import r2_score
r2_inc_lm = r2_score(y1_test, y1_pred_lm)
print(r2_inc_lm)

##check R-square
r2_inc_lm1 = np.corrcoef(y1_test, y1_pred_lm)[0,1]**2
print(r2_inc_lm1)

#ASR.Mort
rmse_mort_lm = np.sqrt(mean_squared_error(y2_test, y2_pred_lm))
print(rmse_mort_lm)

##using MAPE
mape_mort_lm = np.mean(np.abs((y2_test - y2_pred_lm)/y2_test))*100
print(mape_mort_lm)

##using MAAPE
maape_mort_lm = np.mean(np.abs((y2_test - y2_pred_lm) / ((y2_test + y2_pred_lm) / 2))) * 100
print(maape_mort_lm)

##using R-MAPE
rmape_mort_lm = np.mean(np.abs((y2_test - y2_pred_lm) / np.maximum(y2_test, y2_pred_lm))) * 100
print(rmape_mort_lm)

##check R-square
from sklearn.metrics import r2_score
r2_mort_lm = r2_score(y2_test, y2_pred_lm)
print(r2_mort_lm)

##check R-square
r2_mort_lm1 = np.corrcoef(y2_test, y2_pred_lm)[0,1]**2
print(r2_mort_lm1)

"""***GLM (Train)***"""

#ASR.inc
t_rmse_inc_glm = np.sqrt(mean_squared_error(y1_train, y1_tpred_glm))
print(t_rmse_inc_glm)

##using MAPE
t_mape_inc_glm = np.mean(np.abs((y1_train - y1_tpred_glm)/y1_train))*100
print(t_mape_inc_glm)

##using MAAPE
t_maape_inc_glm = np.mean(np.abs((y1_train - y1_tpred_glm) / ((y1_train + y1_tpred_glm) / 2))) * 100
print(t_maape_inc_glm)

##using R-MAPE
t_rmape_inc_glm = np.mean(np.abs((y1_train - y1_tpred_glm) / np.maximum(y1_train, y1_tpred_glm))) * 100
print(t_rmape_inc_glm)

##check R-square
from sklearn.metrics import r2_score
t_r2_inc_glm = r2_score(y1_train, y1_tpred_glm)
print(t_r2_inc_glm)

##check R-square
t_r2_inc_glm1 = np.corrcoef(y1_train, y1_tpred_glm)[0,1]**2
print(t_r2_inc_glm1)

#ASR.mort
t_rmse_mort_glm = np.sqrt(mean_squared_error(y2_train, y2_tpred_glm))
print(t_rmse_mort_glm)

##using MAPE
t_mape_mort_glm = np.mean(np.abs((y2_train - y2_tpred_glm)/y2_train))*100
print(t_mape_mort_glm)

##using MAAPE
t_maape_mort_glm = np.mean(np.abs((y2_train - y2_tpred_glm) / ((y2_train + y2_tpred_glm) / 2))) * 100
print(t_maape_mort_glm)

##using R-MAPE
t_rmape_mort_glm = np.mean(np.abs((y2_train - y2_tpred_glm) / np.maximum(y2_train, y2_tpred_glm))) * 100
print(t_rmape_mort_glm)

##check R-square
from sklearn.metrics import r2_score
t_r2_mort_glm = r2_score(y2_train, y2_tpred_glm)
print(t_r2_mort_glm)

##check R-square
t_r2_mort_glm1 = np.corrcoef(y2_train, y2_tpred_glm)[0,1]**2
print(t_r2_mort_glm1)

"""***GLM (Test)***"""

#ASR.inc
rmse_inc_glm = np.sqrt(mean_squared_error(y1_test, y1_pred_glm))
print(rmse_inc_glm)

##using MAPE
mape_inc_glm = np.mean(np.abs((y1_test - y1_pred_glm)/y1_test))*100
print(mape_inc_glm)

##using MAAPE
maape_inc_glm = np.mean(np.abs((y1_test - y1_pred_glm) / ((y1_test + y1_pred_glm) / 2))) * 100
print(maape_inc_glm)

##using R-MAPE
rmape_inc_glm = np.mean(np.abs((y1_test - y1_pred_glm) / np.maximum(y1_test, y1_pred_glm))) * 100
print(rmape_inc_glm)

##check R-square
from sklearn.metrics import r2_score
r2_inc_glm = r2_score(y1_test, y1_pred_glm)
print(r2_inc_glm)

##check R-square
r2_inc_glm1 = np.corrcoef(y1_test, y1_pred_glm)[0,1]**2
print(r2_inc_glm1)

#ASR.mort
rmse_mort_glm = np.sqrt(mean_squared_error(y2_test, y2_pred_glm))
print(rmse_mort_glm)

##using MAPE
mape_mort_glm = np.mean(np.abs((y2_test - y2_pred_glm)/y2_test))*100
print(mape_mort_glm)

##using MAAPE
maape_mort_glm = np.mean(np.abs((y2_test - y2_pred_glm) / ((y2_test + y2_pred_glm) / 2))) * 100
print(maape_mort_glm)

##using R-MAPE
rmape_mort_glm = np.mean(np.abs((y2_test - y2_pred_glm) / np.maximum(y2_test, y2_pred_glm))) * 100
print(rmape_mort_glm)

##check R-square
from sklearn.metrics import r2_score
r2_mort_glm = r2_score(y2_test, y2_pred_glm)
print(r2_mort_glm)

##check R-square
r2_mort_glm1 = np.corrcoef(y2_test, y2_pred_glm)[0,1]**2
print(r2_mort_glm1)

"""***Regression Tree (Train)***"""

#ASR.inc (Train)
t_rmse_inc_rt = np.sqrt(mean_squared_error(y1_train, y1_tpred_rt))
print(t_rmse_inc_rt)

##using MAPE
t_mape_inc_rt = np.mean(np.abs((y1_train - y1_tpred_rt)/y1_train))*100
print(t_mape_inc_rt)

##using MAAPE
t_maape_inc_rt = np.mean(np.abs((y1_train - y1_tpred_rt) / ((y1_train + y1_tpred_rt) / 2))) * 100
print(t_maape_inc_rt)

##using R-MAPE
t_rmape_inc_rt = np.mean(np.abs((y1_train - y1_tpred_rt) / np.maximum(y1_train, y1_tpred_rt))) * 100
print(t_rmape_inc_rt)

##check R-square
from sklearn.metrics import r2_score
t_r2_inc_rt = r2_score(y1_train, y1_tpred_rt)
print(t_r2_inc_rt)

##check R-square
t_r2_inc_rt1 = np.corrcoef(y1_train, y1_tpred_rt)[0,1]**2
print(t_r2_inc_rt1)

#ASR.mort
t_rmse_mort_rt = np.sqrt(mean_squared_error(y2_train, y2_tpred_rt))
print(t_rmse_mort_rt)

##using MAPE
t_mape_mort_rt = np.mean(np.abs((y2_train - y2_tpred_rt)/y2_train))*100
print(t_mape_mort_rt)

##using MAAPE
t_maape_mort_rt = np.mean(np.abs((y2_train - y2_tpred_rt) / ((y2_train + y2_tpred_rt) / 2))) * 100
print(t_maape_mort_rt)

##using R-MAPE
t_rmape_mort_rt = np.mean(np.abs((y2_train - y2_tpred_rt) / np.maximum(y2_train, y2_tpred_rt))) * 100
print(t_rmape_mort_rt)

##check R-square
from sklearn.metrics import r2_score
t_r2_mort_rt = r2_score(y2_train, y2_tpred_rt)
print(t_r2_mort_rt)

##check R-square
t_r2_mort_rt1 = np.corrcoef(y2_train, y2_tpred_rt)[0,1]**2
print(t_r2_mort_rt1)

"""***Regression Tree (Test)***"""

#ASR.inc
rmse_inc_rt = np.sqrt(mean_squared_error(y1_test, y1_pred_rt))
print(rmse_inc_rt)

##using MAPE
mape_inc_rt = np.mean(np.abs((y1_test - y1_pred_rt)/y1_test))*100
print(mape_inc_rt)

##using MAAPE
maape_inc_rt = np.mean(np.abs((y1_test - y1_pred_rt) / ((y1_test + y1_pred_rt) / 2))) * 100
print(maape_inc_rt)

##using R-MAPE
rmape_inc_rt = np.mean(np.abs((y1_test - y1_pred_rt) / np.maximum(y1_test, y1_pred_rt))) * 100
print(rmape_inc_rt)

##check R-square
from sklearn.metrics import r2_score
r2_inc_rt = r2_score(y1_test, y1_pred_rt)
print(r2_inc_rt)

##check R-square
r2_inc_rt1 = np.corrcoef(y1_test, y1_pred_rt)[0,1]**2
print(r2_inc_rt1)

#ASR.mort
rmse_mort_rt = np.sqrt(mean_squared_error(y2_test, y2_pred_rt))
print(rmse_mort_rt)

##using MAPE
mape_mort_rt = np.mean(np.abs((y2_test - y2_pred_rt)/y2_test))*100
print(mape_mort_rt)

##using MAAPE
maape_mort_rt = np.mean(np.abs((y2_test - y2_pred_rt) / ((y2_test + y2_pred_rt) / 2))) * 100
print(maape_mort_rt)

##using R-MAPE
rmape_mort_rt = np.mean(np.abs((y2_test - y2_pred_rt) / np.maximum(y2_test, y2_pred_rt))) * 100
print(rmape_mort_rt)

##check R-square
from sklearn.metrics import r2_score
r2_mort_rt = r2_score(y2_test, y2_pred_rt)
print(r2_mort_rt)

##check R-square
r2_mort_rt1 = np.corrcoef(y2_test, y2_pred_rt)[0,1]**2
print(r2_mort_rt1)

"""***Random Forest (Train)***"""

#ASR.inc
t_rmse_inc_rf = np.sqrt(mean_squared_error(y1_train, y1_tpred_rf))
print(t_rmse_inc_rf)

##using MAPE
t_mape_inc_rf = np.mean(np.abs((y1_train - y1_tpred_rf)/y1_train))*100
print(t_mape_inc_rf)

##using MAAPE
t_maape_inc_rf = np.mean(np.abs((y1_train - y1_tpred_rf) / ((y1_train + y1_tpred_rf) / 2))) * 100
print(t_maape_inc_rf)

##using R-MAPE
t_rmape_inc_rf = np.mean(np.abs((y1_train - y1_tpred_rf) / np.maximum(y1_train, y1_tpred_rf))) * 100
print(t_rmape_inc_rf)

##check R-square
from sklearn.metrics import r2_score
t_r2_inc_rf = r2_score(y1_train, y1_tpred_rf)
print(t_r2_inc_rf)

##check R-square
t_r2_inc_rf1 = np.corrcoef(y1_train, y1_tpred_rf)[0,1]**2
print(t_r2_inc_rf1)

#ASR.mort
t_rmse_mort_rf = np.sqrt(mean_squared_error(y2_train, y2_tpred_rf))
print(t_rmse_mort_rf)

##using MAPE
t_mape_mort_rf = np.mean(np.abs((y2_train - y2_tpred_rf)/y2_train))*100
print(t_mape_mort_rf)

##using MAAPE
t_maape_mort_rf = np.mean(np.abs((y2_train - y2_tpred_rf) / ((y2_train + y2_tpred_rf) / 2))) * 100
print(t_maape_mort_rf)

##using R-MAPE
t_rmape_mort_rf = np.mean(np.abs((y2_train - y2_tpred_rf) / np.maximum(y2_train, y2_tpred_rf))) * 100
print(t_rmape_mort_rf)

##check R-square
from sklearn.metrics import r2_score
t_r2_mort_rf = r2_score(y2_train, y2_tpred_rf)
print(t_r2_mort_rf)

##check R-square
t_r2_mort_rf1 = np.corrcoef(y2_train, y2_tpred_rf)[0,1]**2
print(t_r2_mort_rf1)

"""***Random Forest (Test)***"""

#ASR.inc
rmse_inc_rf = np.sqrt(mean_squared_error(y1_test, y1_pred_rf))
print(rmse_inc_rf)

##using MAPE
mape_inc_rf = np.mean(np.abs((y1_test - y1_pred_rf)/y1_test))*100
print(mape_inc_rf)

##using MAAPE
maape_inc_rf = np.mean(np.abs((y1_test - y1_pred_rf) / ((y1_test + y1_pred_rf) / 2))) * 100
print(maape_inc_rf)

##using R-MAPE
rmape_inc_rf = np.mean(np.abs((y1_test - y1_pred_rf) / np.maximum(y1_test, y1_pred_rf))) * 100
print(rmape_inc_rf)

##check R-square
from sklearn.metrics import r2_score
r2_inc_rf = r2_score(y1_test, y1_pred_rf)
print(r2_inc_rf)

##check R-square
r2_inc_rf1 = np.corrcoef(y1_test, y1_pred_rf)[0,1]**2
print(r2_inc_rf1)

#ASR.mort
rmse_mort_rf = np.sqrt(mean_squared_error(y2_test, y2_pred_rf))
print(rmse_mort_rf)

##using MAPE
mape_mort_rf = np.mean(np.abs((y2_test - y2_pred_rf)/y2_test))*100
print(mape_mort_rf)

##using MAAPE
maape_mort_rf = np.mean(np.abs((y2_test - y2_pred_rf) / ((y2_test + y2_pred_rf) / 2))) * 100
print(maape_mort_rf)

##using R-MAPE
rmape_mort_rf = np.mean(np.abs((y2_test - y2_pred_rf) / np.maximum(y2_test, y2_pred_rf))) * 100
print(rmape_mort_rf)

##check R-square
from sklearn.metrics import r2_score
r2_mort_rf = r2_score(y2_test, y2_pred_rf)
print(r2_mort_rf)

##check R-square
r2_mort_rf1 = np.corrcoef(y2_test, y2_pred_rf)[0,1]**2
print(r2_mort_rf1)

"""***XGBoost (Train)***"""

#ASR.inc
t_rmse_inc_xgb = np.sqrt(mean_squared_error(y1_train, y1_tpred_xgb))
print(t_rmse_inc_xgb)

##using MAPE
t_mape_inc_xgb = np.mean(np.abs((y1_train - y1_tpred_xgb)/y1_train))*100
print(t_mape_inc_xgb)

##using MAAPE
t_maape_inc_xgb = np.mean(np.abs((y1_train - y1_tpred_xgb) / ((y1_train + y1_tpred_xgb) / 2))) * 100
print(t_maape_inc_xgb)

##using R-MAPE
t_rmape_inc_xgb = np.mean(np.abs((y1_train - y1_tpred_xgb) / np.maximum(y1_train, y1_tpred_xgb))) * 100
print(t_rmape_inc_xgb)

##check R-square
from sklearn.metrics import r2_score
t_r2_inc_xgb = r2_score(y1_train, y1_tpred_xgb)
print(t_r2_inc_xgb)

##check R-square
t_r2_inc_xgb1 = np.corrcoef(y1_train, y1_tpred_xgb)[0,1]**2
print(t_r2_inc_xgb1)

#ASR.mort
##using RMSE
t_rmse_mort_xgb = np.sqrt(mean_squared_error(y2_train, y2_tpred_xgb))
print(t_rmse_mort_xgb)

##using MAPE
t_mape_mort_xgb = np.mean(np.abs((y2_train - y2_tpred_xgb)/y2_train))*100
print(t_mape_mort_xgb)

##using MAAPE
t_maape_mort_xgb = np.mean(np.abs((y2_train - y2_tpred_xgb) / ((y2_train + y2_tpred_xgb) / 2))) * 100
print(t_maape_mort_xgb)

##using R-MAPE
t_rmape_mort_xgb = np.mean(np.abs((y2_train - y2_tpred_xgb) / np.maximum(y2_train, y2_tpred_xgb))) * 100
print(t_rmape_mort_xgb)

##check R-square
from sklearn.metrics import r2_score
t_r2_mort_xgb = r2_score(y2_train, y2_tpred_xgb)
print(t_r2_mort_xgb)

##check R-square
t_r2_mort_xgb1 = np.corrcoef(y2_train, y2_tpred_xgb)[0,1]**2
print(t_r2_mort_xgb1)

"""***XGBoost (Test)***"""

#ASR.inc
rmse_inc_xgb = np.sqrt(mean_squared_error(y1_test, y1_pred_xgb))
print(rmse_inc_xgb)

##using MAPE
mape_inc_xgb = np.mean(np.abs((y1_test - y1_pred_xgb)/y1_test))*100
print(mape_inc_xgb)

##using MAAPE
maape_inc_xgb = np.mean(np.abs((y1_test - y1_pred_xgb) / ((y1_test + y1_pred_xgb) / 2))) * 100
print(maape_inc_xgb)

##using R-MAPE
rmape_inc_xgb = np.mean(np.abs((y1_test - y1_pred_xgb) / np.maximum(y1_test, y1_pred_xgb))) * 100
print(rmape_inc_xgb)

##check R-square
from sklearn.metrics import r2_score
r2_inc_xgb = r2_score(y1_test, y1_pred_xgb)
print(r2_inc_xgb)

##check R-square
r2_inc_xgb1 = np.corrcoef(y1_test, y1_pred_xgb)[0,1]**2
print(r2_inc_xgb1)

#ASR.mort
##using RMSE
rmse_mort_xgb = np.sqrt(mean_squared_error(y2_test, y2_pred_xgb))
print(rmse_mort_xgb)

##using MAPE
mape_mort_xgb = np.mean(np.abs((y2_test - y2_pred_xgb)/y2_test))*100
print(mape_mort_xgb)

##using MAAPE
maape_mort_xgb = np.mean(np.abs((y2_test - y2_pred_xgb) / ((y2_test + y2_pred_xgb) / 2))) * 100
print(maape_mort_xgb)

##using R-MAPE
rmape_mort_xgb = np.mean(np.abs((y2_test - y2_pred_xgb) / np.maximum(y2_test, y2_pred_xgb))) * 100
print(rmape_mort_xgb)

##check R-square
from sklearn.metrics import r2_score
r2_mort_xgb = r2_score(y2_test, y2_pred_xgb)
print(r2_mort_xgb)

##check R-square
r2_mort_xgb1 = np.corrcoef(y2_test, y2_pred_xgb)[0,1]**2
print(r2_mort_xgb1)

"""

---


**SHAP (SHapley Additive exPlanations) VALUES**"""

!pip install shap
import shap

#Regression Tree (ASR Inc)
explainer_inc_rt = shap.TreeExplainer(best_rt_model1)

#calculate shap values. This is what we will plot.
#Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values_inc_rt = explainer_inc_rt.shap_values(x1_test)

#make plot
shap.summary_plot(shap_values_inc_rt, x1_test)

#Regression Tree (ASR Mort)
explainer_mort_rt = shap.TreeExplainer(best_rt_model2)

#calculate shap values. This is what we will plot.
#Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values_mort_rt = explainer_mort_rt.shap_values(x2_test)

#make plot
shap.summary_plot(shap_values_mort_rt, x2_test)

#Random Forest (ASR Inc)
explainer_inc_rf = shap.TreeExplainer(best_rf_model1)

#calculate shap values. This is what we will plot.
#Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values_inc_rf = explainer_inc_rf.shap_values(x1_test)

#make plot
shap.summary_plot(shap_values_inc_rf, x1_test)

#Random Forest (ASR Mort)
explainer_mort_rf = shap.TreeExplainer(best_rf_model2)

#calculate shap values. This is what we will plot.
#Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values_mort_rf = explainer_mort_rt.shap_values(x2_test)

#make plot
shap.summary_plot(shap_values_mort_rf, x2_test)

#XGBoost
explainer = shap.TreeExplainer(best_xgb_model1)

#calculate shap values. This is what we will plot.
#Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values1 = explainer.shap_values(x1_test)

#make plot
shap.summary_plot(shap_values1, x1_test)

explainer = shap.TreeExplainer(best_xgb_model2)

#calculate shap values. This is what we will plot.
#Calculate shap_values for all of val_X rather than a single row, to have more data for plot.
shap_values2 = explainer.shap_values(x2_test)

#make plot
shap.summary_plot(shap_values2, x2_test)